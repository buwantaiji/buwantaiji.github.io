<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hao Xie</title>
    <description>Ph.D. candidate at IOP, CAS</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 15 Oct 2019 15:13:37 +0800</pubDate>
    <lastBuildDate>Tue, 15 Oct 2019 15:13:37 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>The adjoint method and its application in back-propagation of dominant eigen-decomposition</title>
        <description>&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;   MathJax.Hub.Config({     TeX: { equationNumbers: { autoNumber: &quot;all&quot; } }   }); &lt;/script&gt;

&lt;p&gt;The adjoint method provides a new way of thinking about and deriving  formulas for certain computation process. In this article, we will present the basic ideas of the adjoint method and, as a concrete example, demonstrate its consequences in back-propagation of the dominant eigen-decomposition and its relation with the traditional approach.&lt;/p&gt;

&lt;p&gt;$\renewcommand{\vec}[1]{\mathbf{#1}}$&lt;/p&gt;

&lt;h2 id=&quot;basic-ideas-of-the-adjoint-method&quot;&gt;Basic ideas of the adjoint method&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;Consider a simple computation process, in which the setting is as follows. Let $\vec{p} = (p_1,\cdots, p_P)$ be a $P$-dimensional input vector of parameters, and the output $\vec{x} = (x_1, \cdots, x_M)^T$ is a $M$-dimensional (column)  vector. They are related by $M$ equations of the form $f_i(\vec{x}, \vec{p}) = 0$, where $i$ ranges from $1$ to $M$.&lt;/p&gt;

&lt;p&gt;What we need is the back-propagation of this process, in which the adjoint $\overline{\vec{p}} \equiv \frac{\partial \mathcal{L}}{\partial \vec{p}}$ of input is expressed as a function of the adjoint $\overline{\vec{x}} \equiv \frac{\partial \mathcal{L}}{\partial \vec{x}}$ of output. We have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\overline{p_\mu} = \overline{\vec{x}}^T \frac{\partial \vec{x}}{\partial p_\mu}, 
\quad \forall \mu = 1, \cdots, P.
\label{eq: adjointp}&lt;/script&gt;

&lt;p&gt;where the $M$-dimensional column vector $\frac{\partial \vec{x}}{\partial p_\mu}$ is determined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial f_i}{\partial p_\mu} + 
\frac{\partial f_i}{\partial \vec{x}} \frac{\partial \vec{x}}{\partial p_\mu} = 0, 
\quad \forall i = 1, \cdots, M.&lt;/script&gt;

&lt;p&gt;Or, we can express this in a more compact form as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial f}{\partial p_\mu} + 
\frac{\partial f}{\partial \vec{x}} \frac{\partial \vec{x}}{\partial p_\mu} = 0.
\label{eq: partialxpartialp}&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial f}{\partial p_\mu} = 
\begin{pmatrix}
	\frac{\partial f_1}{\partial p_\mu} \\ \vdots \\
	\frac{\partial f_M}{\partial p_\mu}
\end{pmatrix}, \quad
\frac{\partial f}{\partial \vec{x}} = 
\begin{pmatrix}
	\text{—} &amp; \frac{\partial f_1}{\partial \vec{x}} &amp; \text{—} \\
	&amp; \vdots \\
	\text{—} &amp; \frac{\partial f_M}{\partial \vec{x}} &amp; \text{—}
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Assuming the $M \times M$ matrix $\frac{\partial f}{\partial \vec{x}}$ is invertible, one can solve for $\frac{\partial \vec{x}}{\partial p_\mu}$ from Eq. $\eqref{eq: partialxpartialp}$, then substitute it back into Eq. $\eqref{eq: adjointp}$ to get the final result:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\overline{p_\mu} &amp;= -\overline{\vec{x}}^T \left( \frac{\partial f}{\partial \vec{x}} 	\right)^{-1} \frac{\partial f}{\partial p_\mu} \\
    				 &amp;= -\boldsymbol{\lambda}^T \frac{\partial f}{\partial p_\mu}.
	\quad \forall \mu = 1, \cdots, P.
	\label{eq: adjointp in general case}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the column vector $\boldsymbol{\lambda}$ is determined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left( \frac{\partial f}{\partial \vec{x}} 	\right)^T \boldsymbol{\lambda} = 
\overline{\vec{x}}.
\label{eq: lambda in general case}&lt;/script&gt;

&lt;h2 id=&quot;example-dominant-eigen-decomposition&quot;&gt;Example: dominant eigen-decomposition&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;As a concrete example of the adjoint method, consider the dominant eigen-decomposition process, in which a certain eigenvalue $\alpha$ and corresponding eigenvector $\vec{x}$ of $A$ is returned (assuming the eigenvalue is non-degenerate), where $A = A(\vec{p})$ is a $N \times N$ real symmetric matrix depending on the parameter $\vec{p}$.&lt;/p&gt;

&lt;p&gt;Note that in the current case, the output can be effectively treated as a vector of dimension $N + 1$. The correspondence with the general notations in the last section is thus as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\vec{x} \rightarrow \begin{pmatrix}
						\vec{x} \\ \alpha
					\end{pmatrix}, \quad
\overline{\vec{x}} \rightarrow \begin{pmatrix}
						\overline{\vec{x}} \\ \overline{\alpha}
					\end{pmatrix}, \quad
\boldsymbol{\lambda} \rightarrow \begin{pmatrix}
									\boldsymbol{\lambda} \\ k
								 \end{pmatrix}, \quad
\frac{\partial f}{\partial p_\mu} \rightarrow
\begin{pmatrix}
	\frac{\partial f_1}{\partial p_\mu} \\ \vdots \\
	\frac{\partial f_N}{\partial p_\mu} \\
	\frac{\partial f_0}{\partial p_\mu}
\end{pmatrix}, \\
\left( \frac{\partial f}{\partial \vec{x}} 	\right)^T = 
\begin{pmatrix}
	\vert &amp; &amp; \vert \\
	\left( \frac{\partial f_1}{\partial \vec{x}} \right)^T &amp; \cdots &amp; 
	\left( \frac{\partial f_M}{\partial \vec{x}} \right)^T \\
	\vert &amp; &amp; \vert
\end{pmatrix} \rightarrow
\left(\begin{array}{ccc|c}
	\vert &amp; &amp; \vert &amp; \vert \\
	\left( \frac{\partial f_1}{\partial \vec{x}} \right)^T &amp; \cdots &amp; 
	\left( \frac{\partial f_N}{\partial \vec{x}} \right)^T &amp; 
	\left( \frac{\partial f_0}{\partial \vec{x}} \right)^T \\
	\vert &amp; &amp; \vert &amp; \vert \\ \hline
	\frac{\partial f_1}{\partial \alpha} &amp; \cdots &amp; 
	\frac{\partial f_N}{\partial \alpha} &amp; 
	\frac{\partial f_0}{\partial \alpha}
\end{array}\right). %]]&gt;&lt;/script&gt;

&lt;p&gt;The $N + 1$ equations $f_i(\vec{x}, \alpha, \vec{p}) = 0$ are given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_i(\vec{x}, \alpha, \vec{p}) = (A - \alpha I)_i^T \vec{x}, \quad \forall i = 1, \cdots, N. \\
f_0(\vec{x}, \alpha, \vec{p}) = \vec{x}^T \vec{x} - 1.
\label{eq: fs}&lt;/script&gt;

&lt;p&gt;where the subscript $i$ denotes the $i$th column of the matrix $A - \alpha I$,  and the extra equation $f_0(\vec{x}, \alpha, \vec{p}) = 0$ imposes the normalization constraint. Using Eq. $\eqref{eq: fs}$, one can easily obtain that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left( \frac{\partial f}{\partial \vec{x}} 	\right)^T \rightarrow
\begin{pmatrix}
	A - \alpha I &amp; 2\vec{x} \\
	-\vec{x}^T &amp; 0
\end{pmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;In correspondence with Eq. $\eqref{eq: lambda in general case}$ in the general case, the matrix equation satisfied by $\boldsymbol{\lambda}$ and $k$ thus yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(A - \alpha I) \boldsymbol{\lambda} + 2k\vec{x} = \overline{\vec{x}}. \\
-\vec{x}^T \boldsymbol{\lambda} = \overline{\alpha}.
\label{eq: matrix equation of lambda and k}&lt;/script&gt;

&lt;p&gt;One can solve for $k$ by multiplying both sides of the first equation by $\vec{x}^T$, and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2k = \vec{x}^T \overline{\vec{x}}.&lt;/script&gt;

&lt;p&gt;Substituting it back to Eq. $\eqref{eq: matrix equation of lambda and k}$, one can obtain the unique solution for $\boldsymbol{\lambda}$ as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\lambda} = -\overline{\alpha} \vec{x} + \boldsymbol{\lambda_0}, \quad \textrm{where $\boldsymbol{\lambda_0}$ satisfies} \\
\color{red}{
(A - \alpha I)\boldsymbol{\lambda_0} = (1 - \vec{x}\vec{x}^T) \overline{\vec{x}}, \quad \vec{x}^T \boldsymbol{\lambda_0} = 0. }
\label{lambda0}&lt;/script&gt;

&lt;p&gt;The vector $\boldsymbol{\lambda_0}$ in the equation above can be solved by Conjugate Gradient (CG) method.&lt;/p&gt;

&lt;p&gt;Finally, note that in the current case,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial f}{\partial p_\mu} \rightarrow
\begin{pmatrix}
	\frac{\partial A}{\partial p_\mu} \vec{x} \\ 0
\end{pmatrix}&lt;/script&gt;

&lt;p&gt;In correspondence with Eq. $\eqref{eq: adjointp in general case}$, one thus obtains&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
	\overline{p_\mu} &amp;= -\boldsymbol{\lambda}^T \frac{\partial A}{\partial p_\mu} \vec{x} \\
	&amp;= (\overline{\alpha} \vec{x}^T - \boldsymbol{\lambda_0}^T) \frac{\partial A}{\partial p_\mu} \vec{x}.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Or, one can “strip” the parameter $\vec{p}$ out of the function primitive and obtain the expression of $\overline{A}$ by taking account of the fact that $\overline{p_\mu} = \textrm{Tr}\left(\overline{A}^T \frac{\partial A}{\partial p_\mu}\right)$. The final result is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\color{red}{
\overline{A} = (\overline{\alpha} \vec{x} - \boldsymbol{\lambda_0}) \vec{x}^T. }
\label{result: dominant diagonalization}&lt;/script&gt;

&lt;p&gt;Fairly simple.&lt;/p&gt;

&lt;h3 id=&quot;relation-with-the-derivation-based-on-full-eigen-decomposition&quot;&gt;Relation with the derivation based on full eigen-decomposition&lt;/h3&gt;

&lt;p&gt;Now, let’s try to figure out the relation of the above approach based on adjoint method with that of the full eigen-decomposition process. The only difference is that compared with the full diagonalization formulation, the adjoint method presented above only needs one specific eigenvalue(usually the smallest or the largest one) and corresponding eigenvectors. Nevertheless, we can “wrap” the process of full diagonalization within the dominant eigen-decomposition and utilize the results of the former formulation in the latter, as demonstrated in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/2019-10-14-full_diagonalization.png&quot; alt=&quot;2019-10-14-full_diagonalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For clarity and without loss of generality, let $\alpha$ and $\vec{x}$ be the “first” eigenvalue and corresponding eigenvector of the matrix $A$. That is, we have $U^T A U = D$, where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
D = \begin{pmatrix}
		\alpha \\
		&amp; \alpha_2 \\
		&amp; &amp; \ddots \\
		&amp; &amp; &amp; \alpha_N
	\end{pmatrix}, \quad 
U = \begin{pmatrix}
		\vert &amp; \vert &amp; &amp; \vert \\
		\vec{x} &amp; \vec{x}_2 &amp; \cdots &amp; \vec{x}_N \\
		\vert &amp; \vert &amp; &amp; \vert
	\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Recall that the full eigen-decomposition gives the following backward formula of $\overline{A}$ in terms of $\overline{D}$ and $\overline{U}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\overline{A} = U (\overline{D} \circ I + U^T \overline{U} \circ F) U^T.
\label{eq: full diagonalization AD}&lt;/script&gt;

&lt;p&gt;where $F$ is an anti-symmetric matrix with off-diagonal elements $F_{ij} = (\alpha_j - \alpha_i)^{-1}$. For more details, see the references in the last section. On the other hand, the dominant eigen-decomposition described above yields the following relations between $\overline{D}, \overline{U}$ and $\overline{\alpha}, \overline{\vec{x}}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\overline{D} \circ I = 
	\begin{pmatrix}
		\overline{\alpha} \\
		&amp; 0 \\
		&amp; &amp; \ddots \\
		&amp; &amp; &amp; 0
	\end{pmatrix}, \quad 
\overline{U} = 
	\begin{pmatrix}
	\begin{array}{c|}
		\vert \\
		\overline{\vec{x}} \\
		\vert
	\end{array} &amp; 
    \begin{array}{ccc}
		\\ &amp; \Huge{0} &amp; \\ &amp; 
	\end{array}
	\end{pmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Then one can obtain through simple algebraic manipulation that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\overline{D} \circ I + U^T \overline{U} \circ F = 
\begin{pmatrix}
\begin{array}{c|}
	\overline{\alpha} \\
	\frac{1}{\alpha - \alpha_2} \vec{x}_2^T \overline{\vec{x}} \\
	\vdots \\
	\frac{1}{\alpha - \alpha_N} \vec{x}_N^T \overline{\vec{x}}
\end{array} &amp; 
\begin{array}{ccc}
\\ &amp; \Huge{0} &amp; \\ &amp; 
\end{array}
\end{pmatrix} \equiv 
\begin{pmatrix}
\begin{array}{c|}
	\overline{\alpha} \\
	-c_2 \\
	\vdots \\
	-c_N
\end{array} &amp; 
\begin{array}{ccc}
\\ &amp; \Huge{0} &amp; \\ &amp; 
\end{array}
\end{pmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;where we have introduced the quantities $c_i = \frac{1}{\alpha_i - \alpha} \vec{x}_i^T \overline{\vec{x}}, \quad \forall i = 2, \cdots, N$. Substituting this equation back into $\eqref{eq: full diagonalization AD}$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\overline{A} = \overline{\alpha} \vec{x} \vec{x}^T - \sum_{i=2}^N c_i \vec{x}_i \vec{x}^T.
\label{result: full diagonalization}&lt;/script&gt;

&lt;p&gt;This formula looks very similar to the result $\eqref{result: dominant diagonalization}$ obtained from the adjoint method. Actually they are &lt;strong&gt;identically the same&lt;/strong&gt;. This can be seen by expanding the vector $\boldsymbol{\lambda_0}$ characterized in Eq. $\eqref{lambda0}$ in the complete basis $(\vec{x}, \vec{x}_2, \cdots, \vec{x}_N)$. One can easily see that the quantities $c_i$ defined above are exactly the linear combination coefficients of $\boldsymbol{\lambda_0}$ in this basis. In other words, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\lambda_0} = \sum_{i=2}^N c_i \vec{x}_i.&lt;/script&gt;

&lt;p&gt;Plugging this result back into $\eqref{result: full diagonalization}$ clearly reproduces the earlier result $\eqref{result: dominant diagonalization}$.&lt;/p&gt;

&lt;h3 id=&quot;remarks&quot;&gt;Remarks&lt;/h3&gt;

&lt;p&gt;The observation above clarifies the equivalence of the two formulations of deriving the back-propagation of  the dominant eigen-decomposition process. &lt;strong&gt;The fact that the final result is identically the same is not surprising, but the “native” representations of the result are indeed different from a practical point of view&lt;/strong&gt;. In the formulation based on adjoint method, one only makes use of the “dominant” eigenvalue and eigenvector instead of the full spectrum, thus allowing for the construction of a valid dominant eigensolver function primitive in the framework of automatic differentiation. In a typical implementation, the forward pass can be accomplished by Lanczos or other dominant diagonalization algorithms, while the backward pass can be implemented by solving the linear system $\eqref{lambda0}$ using Conjugate Gradient method. In the case of large matrix dimension, this approach is clearly more efficient.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;The presentation of general ideas of adjoint method and its application in dominant eigen-decomposition is largely based on this notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://math.mit.edu/~stevenj/18.336/adjoint.pdf&quot;&gt;https://math.mit.edu/~stevenj/18.336/adjoint.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some good notes on automatic differentiation of basic matrix operations, including the full eigen-decomposition process. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf&quot;&gt;https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 15 Oct 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/10/adjoint-method/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/10/adjoint-method/</guid>
        
        
      </item>
    
      <item>
        <title>Hello World - Vno</title>
        <description>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;[toc]&lt;/p&gt;
&lt;h2 id=&quot;this-is-a-2nd-title&quot;&gt;This is a 2nd title.&lt;/h2&gt;
&lt;h4 id=&quot;whats-this&quot;&gt;What’s this&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/onevcat/vno-jekyll&quot;&gt;Vno Jekyll&lt;/a&gt; is a theme for &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt;. It is a port of my Ghost theme &lt;a href=&quot;https://github.com/onevcat/vno&quot;&gt;vno&lt;/a&gt;, which is originally developed from &lt;a href=&quot;https://github.com/daleanthony/uno&quot;&gt;Dale Anthony’s Uno&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;usage&quot;&gt;Usage&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/onevcat/vno-jekyll.git your_site
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;your_site
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bundler &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bundler &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;jekyll serve
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello, world!&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sin(x + y) = \sin(x) \cos(y) + \cos(x) \sin(y)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{V}_1 \times \mathbf{V}_2 =  \begin{vmatrix}
\mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\
\frac{\partial X}{\partial u} &amp;  \frac{\partial Y}{\partial u} &amp; 0 \\
\frac{\partial X}{\partial v} &amp;  \frac{\partial Y}{\partial v} &amp; 0 \\
\end{vmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Your site with &lt;code class=&quot;highlighter-rouge&quot;&gt;Vno Jekyll&lt;/code&gt; enabled should be accessible in http://127.0.0.1:4000.&lt;/p&gt;

&lt;p&gt;For more information about Jekyll, please visit &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll’s site&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;

&lt;p&gt;All configuration could be done in &lt;code class=&quot;highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;. Remember you need to restart to serve the page when after changing the config file. Everything in the config file should be self-explanatory.&lt;/p&gt;

&lt;h4 id=&quot;background-image-and-avatar&quot;&gt;Background image and avatar&lt;/h4&gt;

&lt;p&gt;You could replace the background and avatar image in &lt;code class=&quot;highlighter-rouge&quot;&gt;assets/images&lt;/code&gt; folder to change them.&lt;/p&gt;

&lt;h4 id=&quot;sites-using-vno&quot;&gt;Sites using Vno&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://onevcat.com&quot;&gt;My blog&lt;/a&gt; is using &lt;code class=&quot;highlighter-rouge&quot;&gt;Vno Jekyll&lt;/code&gt; as well, you could see how it works in real. There are some other sites using the same theme. You can find them below:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Site Name&lt;/th&gt;
      &lt;th&gt;URL&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;OneV’s Den&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://onevcat.com&quot;&gt;http://onevcat.com&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;July Tang&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://onevcat.com&quot;&gt;http://blog.julytang.xyz&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Harry Lee&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://qiuqi.li&quot;&gt;http://qiuqi.li&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you happen to be using this theme, welcome to &lt;a href=&quot;https://github.com/onevcat/vno-jekyll/pulls&quot;&gt;send me a pull request&lt;/a&gt; to add your site link here. :)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;license&quot;&gt;License&lt;/h4&gt;

&lt;p&gt;Great thanks to &lt;a href=&quot;https://github.com/daleanthony&quot;&gt;Dale Anthony&lt;/a&gt; and his &lt;a href=&quot;https://github.com/daleanthony/uno&quot;&gt;Uno&lt;/a&gt;. Vno Jekyll is based on Uno, and contains a lot of modification on page layout, animation, font and some more things I can not remember. Vno Jekyll is followed with Uno and be licensed as &lt;a href=&quot;http://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International&lt;/a&gt;. See the link for more information.&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Feb 2016 14:32:24 +0800</pubDate>
        <link>http://localhost:4000/2016/02/hello-world-vno/</link>
        <guid isPermaLink="true">http://localhost:4000/2016/02/hello-world-vno/</guid>
        
        
      </item>
    
  </channel>
</rss>
