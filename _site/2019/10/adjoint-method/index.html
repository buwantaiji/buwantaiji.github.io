<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>The adjoint method and its application in back-propagation of dominant eigen-decomposition</title>
  <meta name="description" content="">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="The adjoint method and its application in back-propagation of dominant eigen-decomposition">
  <meta name="twitter:description" content="">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="The adjoint method and its application in back-propagation of dominant eigen-decomposition">
  <meta property="og:description" content="">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/2019/10/adjoint-method/">
  <link rel="alternate" type="application/rss+xml" title="Hao Xie" href="http://localhost:4000/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 Hao Xie 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="Hao Xie logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for Hao Xie" class="blog-button">Hao Xie</a></h1>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">Ph.D. candidate at IOP, CAS</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/buwantaiji" title="@buwantaiji 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  

  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:xiehao18@iphy.ac.cn" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-disabled"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2019-10-15 00:00:00 +0800" itemprop="datePublished" class="post-meta__date date">2019-10-15</time> &#8226; <span class="post-meta__tags tags"></span>
    </div>
    <h1 class="post-title">The adjoint method and its application in back-propagation of dominant eigen-decomposition</h1>
  </header>

  <section class="post">
    <script type="text/javascript" async="" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>

<script type="text/x-mathjax-config">   MathJax.Hub.Config({     TeX: { equationNumbers: { autoNumber: "all" } }   }); </script>

<p>The adjoint method provides a new way of thinking about and deriving  formulas for certain computation process. In this article, we will present the basic ideas of the adjoint method and, as a concrete example, demonstrate its consequences in back-propagation of the dominant eigen-decomposition and its relation with the traditional approach.</p>

<p>$\renewcommand{\vec}[1]{\mathbf{#1}}$</p>

<h2 id="basic-ideas-of-the-adjoint-method">Basic ideas of the adjoint method</h2>

<hr />

<p>Consider a simple computation process, in which the setting is as follows. Let $\vec{p} = (p_1,\cdots, p_P)$ be a $P$-dimensional input vector of parameters, and the output $\vec{x} = (x_1, \cdots, x_M)^T$ is a $M$-dimensional (column)  vector. They are related by $M$ equations of the form $f_i(\vec{x}, \vec{p}) = 0$, where $i$ ranges from $1$ to $M$.</p>

<p>What we need is the back-propagation of this process, in which the adjoint $\overline{\vec{p}} \equiv \frac{\partial \mathcal{L}}{\partial \vec{p}}$ of input is expressed as a function of the adjoint $\overline{\vec{x}} \equiv \frac{\partial \mathcal{L}}{\partial \vec{x}}$ of output. We have</p>

<script type="math/tex; mode=display">\overline{p_\mu} = \overline{\vec{x}}^T \frac{\partial \vec{x}}{\partial p_\mu}, 
\quad \forall \mu = 1, \cdots, P.
\label{eq: adjointp}</script>

<p>where the $M$-dimensional column vector $\frac{\partial \vec{x}}{\partial p_\mu}$ is determined by</p>

<script type="math/tex; mode=display">\frac{\partial f_i}{\partial p_\mu} + 
\frac{\partial f_i}{\partial \vec{x}} \frac{\partial \vec{x}}{\partial p_\mu} = 0, 
\quad \forall i = 1, \cdots, M.</script>

<p>Or, we can express this in a more compact form as follows:</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial p_\mu} + 
\frac{\partial f}{\partial \vec{x}} \frac{\partial \vec{x}}{\partial p_\mu} = 0.
\label{eq: partialxpartialp}</script>

<p>where</p>

<script type="math/tex; mode=display">% <![CDATA[
\frac{\partial f}{\partial p_\mu} = 
\begin{pmatrix}
	\frac{\partial f_1}{\partial p_\mu} \\ \vdots \\
	\frac{\partial f_M}{\partial p_\mu}
\end{pmatrix}, \quad
\frac{\partial f}{\partial \vec{x}} = 
\begin{pmatrix}
	\text{—} & \frac{\partial f_1}{\partial \vec{x}} & \text{—} \\
	& \vdots \\
	\text{—} & \frac{\partial f_M}{\partial \vec{x}} & \text{—}
\end{pmatrix} %]]></script>

<p>Assuming the $M \times M$ matrix $\frac{\partial f}{\partial \vec{x}}$ is invertible, one can solve for $\frac{\partial \vec{x}}{\partial p_\mu}$ from Eq. $\eqref{eq: partialxpartialp}$, then substitute it back into Eq. $\eqref{eq: adjointp}$ to get the final result:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\overline{p_\mu} &= -\overline{\vec{x}}^T \left( \frac{\partial f}{\partial \vec{x}} 	\right)^{-1} \frac{\partial f}{\partial p_\mu} \\
    				 &= -\boldsymbol{\lambda}^T \frac{\partial f}{\partial p_\mu}.
	\quad \forall \mu = 1, \cdots, P.
	\label{eq: adjointp in general case}
\end{align} %]]></script>

<p>where the column vector $\boldsymbol{\lambda}$ is determined by</p>

<script type="math/tex; mode=display">\left( \frac{\partial f}{\partial \vec{x}} 	\right)^T \boldsymbol{\lambda} = 
\overline{\vec{x}}.
\label{eq: lambda in general case}</script>

<h2 id="example-dominant-eigen-decomposition">Example: dominant eigen-decomposition</h2>

<hr />

<p>As a concrete example of the adjoint method, consider the dominant eigen-decomposition process, in which a certain eigenvalue $\alpha$ and corresponding eigenvector $\vec{x}$ of $A$ is returned (assuming the eigenvalue is non-degenerate), where $A = A(\vec{p})$ is a $N \times N$ real symmetric matrix depending on the parameter $\vec{p}$.</p>

<p>Note that in the current case, the output can be effectively treated as a vector of dimension $N + 1$. The correspondence with the general notations in the last section is thus as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\vec{x} \rightarrow \begin{pmatrix}
						\vec{x} \\ \alpha
					\end{pmatrix}, \quad
\overline{\vec{x}} \rightarrow \begin{pmatrix}
						\overline{\vec{x}} \\ \overline{\alpha}
					\end{pmatrix}, \quad
\boldsymbol{\lambda} \rightarrow \begin{pmatrix}
									\boldsymbol{\lambda} \\ k
								 \end{pmatrix}, \quad
\frac{\partial f}{\partial p_\mu} \rightarrow
\begin{pmatrix}
	\frac{\partial f_1}{\partial p_\mu} \\ \vdots \\
	\frac{\partial f_N}{\partial p_\mu} \\
	\frac{\partial f_0}{\partial p_\mu}
\end{pmatrix}, \\
\left( \frac{\partial f}{\partial \vec{x}} 	\right)^T = 
\begin{pmatrix}
	\vert & & \vert \\
	\left( \frac{\partial f_1}{\partial \vec{x}} \right)^T & \cdots & 
	\left( \frac{\partial f_M}{\partial \vec{x}} \right)^T \\
	\vert & & \vert
\end{pmatrix} \rightarrow
\left(\begin{array}{ccc|c}
	\vert & & \vert & \vert \\
	\left( \frac{\partial f_1}{\partial \vec{x}} \right)^T & \cdots & 
	\left( \frac{\partial f_N}{\partial \vec{x}} \right)^T & 
	\left( \frac{\partial f_0}{\partial \vec{x}} \right)^T \\
	\vert & & \vert & \vert \\ \hline
	\frac{\partial f_1}{\partial \alpha} & \cdots & 
	\frac{\partial f_N}{\partial \alpha} & 
	\frac{\partial f_0}{\partial \alpha}
\end{array}\right). %]]></script>

<p>The $N + 1$ equations $f_i(\vec{x}, \alpha, \vec{p}) = 0$ are given by</p>

<script type="math/tex; mode=display">f_i(\vec{x}, \alpha, \vec{p}) = (A - \alpha I)_i^T \vec{x}, \quad \forall i = 1, \cdots, N. \\
f_0(\vec{x}, \alpha, \vec{p}) = \vec{x}^T \vec{x} - 1.
\label{eq: fs}</script>

<p>where the subscript $i$ denotes the $i$th column of the matrix $A - \alpha I$,  and the extra equation $f_0(\vec{x}, \alpha, \vec{p}) = 0$ imposes the normalization constraint. Using Eq. $\eqref{eq: fs}$, one can easily obtain that</p>

<script type="math/tex; mode=display">% <![CDATA[
\left( \frac{\partial f}{\partial \vec{x}} 	\right)^T \rightarrow
\begin{pmatrix}
	A - \alpha I & 2\vec{x} \\
	-\vec{x}^T & 0
\end{pmatrix}. %]]></script>

<p>In correspondence with Eq. $\eqref{eq: lambda in general case}$ in the general case, the matrix equation satisfied by $\boldsymbol{\lambda}$ and $k$ thus yields</p>

<script type="math/tex; mode=display">(A - \alpha I) \boldsymbol{\lambda} + 2k\vec{x} = \overline{\vec{x}}. \\
-\vec{x}^T \boldsymbol{\lambda} = \overline{\alpha}.
\label{eq: matrix equation of lambda and k}</script>

<p>One can solve for $k$ by multiplying both sides of the first equation by $\vec{x}^T$, and obtain</p>

<script type="math/tex; mode=display">2k = \vec{x}^T \overline{\vec{x}}.</script>

<p>Substituting it back to Eq. $\eqref{eq: matrix equation of lambda and k}$, one can obtain the unique solution for $\boldsymbol{\lambda}$ as follows:</p>

<script type="math/tex; mode=display">\boldsymbol{\lambda} = -\overline{\alpha} \vec{x} + \boldsymbol{\lambda_0}, \quad \textrm{where $\boldsymbol{\lambda_0}$ satisfies} \\
\color{red}{
(A - \alpha I)\boldsymbol{\lambda_0} = (1 - \vec{x}\vec{x}^T) \overline{\vec{x}}, \quad \vec{x}^T \boldsymbol{\lambda_0} = 0. }
\label{lambda0}</script>

<p>The vector $\boldsymbol{\lambda_0}$ in the equation above can be solved by Conjugate Gradient (CG) method.</p>

<p>Finally, note that in the current case,</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial p_\mu} \rightarrow
\begin{pmatrix}
	\frac{\partial A}{\partial p_\mu} \vec{x} \\ 0
\end{pmatrix}</script>

<p>In correspondence with Eq. $\eqref{eq: adjointp in general case}$, one thus obtains</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\overline{p_\mu} &= -\boldsymbol{\lambda}^T \frac{\partial A}{\partial p_\mu} \vec{x} \\
	&= (\overline{\alpha} \vec{x}^T - \boldsymbol{\lambda_0}^T) \frac{\partial A}{\partial p_\mu} \vec{x}.
\end{align} %]]></script>

<p>Or, one can “strip” the parameter $\vec{p}$ out of the function primitive and obtain the expression of $\overline{A}$ by taking account of the fact that $\overline{p_\mu} = \textrm{Tr}\left(\overline{A}^T \frac{\partial A}{\partial p_\mu}\right)$. The final result is</p>

<script type="math/tex; mode=display">\color{red}{
\overline{A} = (\overline{\alpha} \vec{x} - \boldsymbol{\lambda_0}) \vec{x}^T. }
\label{result: dominant diagonalization}</script>

<p>Fairly simple.</p>

<h3 id="relation-with-the-derivation-based-on-full-eigen-decomposition">Relation with the derivation based on full eigen-decomposition</h3>

<p>Now, let’s try to figure out the relation of the above approach based on adjoint method with that of the full eigen-decomposition process. The only difference is that compared with the full diagonalization formulation, the adjoint method presented above only needs one specific eigenvalue(usually the smallest or the largest one) and corresponding eigenvectors. Nevertheless, we can “wrap” the process of full diagonalization within the dominant eigen-decomposition and utilize the results of the former formulation in the latter, as demonstrated in the figure below.</p>

<p><img src="/assets/images/2019-10-14-full_diagonalization.png" alt="2019-10-14-full_diagonalization" /></p>

<p>For clarity and without loss of generality, let $\alpha$ and $\vec{x}$ be the “first” eigenvalue and corresponding eigenvector of the matrix $A$. That is, we have $U^T A U = D$, where</p>

<script type="math/tex; mode=display">% <![CDATA[
D = \begin{pmatrix}
		\alpha \\
		& \alpha_2 \\
		& & \ddots \\
		& & & \alpha_N
	\end{pmatrix}, \quad 
U = \begin{pmatrix}
		\vert & \vert & & \vert \\
		\vec{x} & \vec{x}_2 & \cdots & \vec{x}_N \\
		\vert & \vert & & \vert
	\end{pmatrix} %]]></script>

<p>Recall that the full eigen-decomposition gives the following backward formula of $\overline{A}$ in terms of $\overline{D}$ and $\overline{U}$:</p>

<script type="math/tex; mode=display">\overline{A} = U (\overline{D} \circ I + U^T \overline{U} \circ F) U^T.
\label{eq: full diagonalization AD}</script>

<p>where $F$ is an anti-symmetric matrix with off-diagonal elements $F_{ij} = (\alpha_j - \alpha_i)^{-1}$. For more details, see the references in the last section. On the other hand, the dominant eigen-decomposition described above yields the following relations between $\overline{D}, \overline{U}$ and $\overline{\alpha}, \overline{\vec{x}}$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\overline{D} \circ I = 
	\begin{pmatrix}
		\overline{\alpha} \\
		& 0 \\
		& & \ddots \\
		& & & 0
	\end{pmatrix}, \quad 
\overline{U} = 
	\begin{pmatrix}
	\begin{array}{c|}
		\vert \\
		\overline{\vec{x}} \\
		\vert
	\end{array} & 
    \begin{array}{ccc}
		\\ & \Huge{0} & \\ & 
	\end{array}
	\end{pmatrix}. %]]></script>

<p>Then one can obtain through simple algebraic manipulation that</p>

<script type="math/tex; mode=display">% <![CDATA[
\overline{D} \circ I + U^T \overline{U} \circ F = 
\begin{pmatrix}
\begin{array}{c|}
	\overline{\alpha} \\
	\frac{1}{\alpha - \alpha_2} \vec{x}_2^T \overline{\vec{x}} \\
	\vdots \\
	\frac{1}{\alpha - \alpha_N} \vec{x}_N^T \overline{\vec{x}}
\end{array} & 
\begin{array}{ccc}
\\ & \Huge{0} & \\ & 
\end{array}
\end{pmatrix} \equiv 
\begin{pmatrix}
\begin{array}{c|}
	\overline{\alpha} \\
	-c_2 \\
	\vdots \\
	-c_N
\end{array} & 
\begin{array}{ccc}
\\ & \Huge{0} & \\ & 
\end{array}
\end{pmatrix}. %]]></script>

<p>where we have introduced the quantities $c_i = \frac{1}{\alpha_i - \alpha} \vec{x}_i^T \overline{\vec{x}}, \quad \forall i = 2, \cdots, N$. Substituting this equation back into $\eqref{eq: full diagonalization AD}$, we have</p>

<script type="math/tex; mode=display">\overline{A} = \overline{\alpha} \vec{x} \vec{x}^T - \sum_{i=2}^N c_i \vec{x}_i \vec{x}^T.
\label{result: full diagonalization}</script>

<p>This formula looks very similar to the result $\eqref{result: dominant diagonalization}$ obtained from the adjoint method. Actually they are <strong>identically the same</strong>. This can be seen by expanding the vector $\boldsymbol{\lambda_0}$ characterized in Eq. $\eqref{lambda0}$ in the complete basis $(\vec{x}, \vec{x}_2, \cdots, \vec{x}_N)$. One can easily see that the quantities $c_i$ defined above are exactly the linear combination coefficients of $\boldsymbol{\lambda_0}$ in this basis. In other words, we have</p>

<script type="math/tex; mode=display">\boldsymbol{\lambda_0} = \sum_{i=2}^N c_i \vec{x}_i.</script>

<p>Plugging this result back into $\eqref{result: full diagonalization}$ clearly reproduces the earlier result $\eqref{result: dominant diagonalization}$.</p>

<h3 id="remarks">Remarks</h3>

<p>The observation above clarifies the equivalence of the two formulations of deriving the back-propagation of  the dominant eigen-decomposition process. <strong>The fact that the final result is identically the same is not surprising, but the “native” representations of the result are indeed different from a practical point of view</strong>. In the formulation based on adjoint method, one only makes use of the “dominant” eigenvalue and eigenvector instead of the full spectrum, thus allowing for the construction of a valid dominant eigensolver function primitive in the framework of automatic differentiation. In a typical implementation, the forward pass can be accomplished by Lanczos or other dominant diagonalization algorithms, while the backward pass can be implemented by solving the linear system $\eqref{lambda0}$ using Conjugate Gradient method. In the case of large matrix dimension, this approach is clearly more efficient.</p>

<h2 id="references">References</h2>

<hr />

<p>The presentation of general ideas of adjoint method and its application in dominant eigen-decomposition is largely based on this notes:</p>

<ul>
  <li><a href="https://math.mit.edu/~stevenj/18.336/adjoint.pdf">https://math.mit.edu/~stevenj/18.336/adjoint.pdf</a></li>
</ul>

<p>There are some good notes on automatic differentiation of basic matrix operations, including the full eigen-decomposition process. For example:</p>

<ul>
  <li><a href="https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf">https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf</a></li>
</ul>

  </section>
</article>

            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2019-10-15 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://onev.cat">@onevcat</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2019</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
